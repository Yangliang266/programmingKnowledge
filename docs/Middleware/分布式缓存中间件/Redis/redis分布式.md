## redis支持分布式

### 为什么要支持分布式

#### 1 高可用

> 可用性和安全的问题。如果只有一个 Redis 服务，一旦服务宕机，那么所有的客户端都无法访问，会对业务造成很大的影响。另一个，如果硬件发生故障，而单机的数据无法恢复的话，带来的影响也是灾难性的。  

#### 2 高性能

> Redis 本身的 QPS 已经很高了，但是如果在一些并发量非常高的情况下，性能还是会受到影响。这个时候我们希望有更多的 Redis 服务来完成工作。  

#### 3 可扩展

> 出于存储的考虑。因为 Redis 所有的数据都放在内存中，如果数据量大，很容易受到硬件的限制。升级硬件收效和成本比太低，所以我们需要有一种横向扩展的方法。  



#### 4 集群

> 可用性、数据安全、性能都可以通过搭建多个 Reids 服务实现。其中有一个是主节（master），可以有多个从节点(slave)。主从之间通过数据同步，存储完全相同的数据。如果主节点发生故障，则把某个从节点改成主节点，访问新的主节点  



## 主从复制

### 1 配置

```
例如一主多从,203 是主节点，在每个 slave 节点的 redis.conf 配置文件增加一行
`slaveof 192.168.8.203 6379`
在主从切换的时候，这个配置会被重写成：
`#Generated by CONFIG REWRITE`
`replicaof 192.168.8.203 6379`
或者在启动服务时通过参数指定 master 节点：
`./redis-server --slaveof 192.168.8.203 6379`
或在客户端直接执行 slaveof xx xx，使该 Redis 实例成为从节点。
启动后，查看集群状态：
`redis> info replication`
从节点不能写入数据（只读），只能从 master 节点同步数据。get 成功，set 失败。
`127.0.0.1:6379> set rr 666`
(error) READONLY You can't write against a read only replica  

主节点写入后，slave 会自动从 master 同步数据。
断开复制：  
`redis> slaveof no one`
此时从节点会变成自己的主节点，不再复制数据。  
```

### 2 主从复制原理

#### 2.1 连接阶段

1. slave node 启动时（执行 slaveof 命令），会在自己本地保存 master node 的信息，包括 master node 的 host 和 ip。

2. slave node 内部有个定时任务 replicationCron（源码 replication.c），每隔 1秒钟检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立socket 网络连接，如果连接成功，从节点为该 socket 建立一个专门处理复制工作的文件事件处理器，负责后续的复制工作，如接收 RDB 文件、接收命令传播等。当从节点变成了主节点的一个客户端之后，会给主节点发送 ping 请求。

   

#### 2.2 数据同步阶段

3. master node 第一次执行全量复制，通过 bgsave 命令在本地生成一份 RDB 快照，将 RDB 快照文件发给 slave node（如果超时会重连，可以调大 repl-timeout 的值）。slave node 首先清除自己的旧数据，然后用 RDB 文件加载数据。



**问题：生成 RDB 期间master 接收到的命令怎么处理？**

- 开始生成 RDB 文件时，master 会把所有新的写命令缓存在内存中。在 slave node  保存了 RDB 之后，再将新的写命令复制给 slave node。  



#### 2.3 命令传播阶段  

4. master node 持续将写命令，异步复制给 slave node延迟是不可避免的，只能通过优化网络。`repl-disable-tcp-nodelay no  `

   当设置为 yes 时，TCP 会对包进行合并从而减少带宽，但是发送的频率会降低，从节点数据延迟增加，一致性变差；具体发送频率与 Linux 内核的配置有关，默认配置为40ms。当设置为 no 时，TCP 会立马将主节点的数据发送给从节点，带宽增加但延迟变小。一般来说，只有当应用对 Redis 数据不一致的容忍度较高，且主从节点之间网络状况不好时，才会设置为 yes；多数情况使用默认值no。

**问题：如果从节点有一段时间断开了与主节点的连接是不是要重新全量复制一遍？**

- 如果可以增量复制，怎么知道上次复制到哪里？通过 master_repl_offset 记录的偏移量  。

- `redis> info replication ` 

#### 2.4 主从复制的不足  

> 主从模式解决了数据备份和性能（通过读写分离）的问题，但是还是存在一些不足：

1. RDB 文件过大的情况下，同步非常耗时。

2. 在一主一从或者一主多从的情况下，如果主服务器挂了，对外提供的服务就不可用了，单点问题没有得到解决。如果每次都是手动把之前的从服务器切换成主服务器，这个比较费时费力，还会造成一定时间的服务不可用 。

   

## 可用性保证之 Sentinel  

#### 1 Sentinel 原理  

> 创建一台监控服务器来监控所有 Redis 服务节点的状态，比如，master 节点超过一定时间没有给监控服务器发送心跳报文，就把 master 标记为下线，然后把某一个 slave变成 master。应用每一次都是从这个监控服务器拿到 master 的地址。
> 问题是：如果监控服务器本身出问题了怎么办？那我们就拿不到 master 的地址了，应用也没有办法访问。
> 那我们再创建一个监控服务器，来监控监控服务器……似乎陷入死循环了，这个问题怎么解决？这个问题先放着。
> <span style="color:red">Redis 的 Sentinel 就是这种思路：通过运行监控服务器来保证服务的可用性。  </span>

> 启动一个或者多个 Sentinel 的服务（通过 src/redis-sentinel），它本质上只是一个运行在特殊模式之下的 Redis，Sentinel 通过 info 命令得到被监听 Redis 机器的master，slave 等信息。  

<img src="https://yliang.oss-cn-shanghai.aliyuncs.com/img/programming/image-20201216160800098.png" alt="image-20201216160800098" style="zoom:50%;" />

> 为了保证监控服务器的可用性，我们会对 Sentinel 做集群的部署。Sentinel 既监控所有的 Redis 服务，<span style="color:red">Sentinel 之间也相互监控。</span>
> 注意：Sentinel 本身没有主从之分，只有 Redis 服务节点有主从之分。概念梳理：master，slave（redis group），sentinel，sentinel 集合 。

##### 1.1 服务下线
Sentinel 默认以每秒钟 1 次的频率向 Redis 服务节点发送 PING 命令。如果在down-after-milliseconds 内都没有收到有效回复，Sentinel 会将该服务器标记为下线（主观下线）。

```
# sentinel.conf
sentinel down-after-milliseconds <master-name> <milliseconds>
```

这个时候 Sentinel 节点会继续询问其他的 Sentinel 节点，确认这个节点是否下线，如果多数 Sentinel 节点都认为 master 下线，master 才真正确认被下线（客观下线），这个时候就需要重新选举 master。  



##### 1.2 故障转移

> 如果 master 被标记为下线，就会开始故障转移流程。既然有这么多的 Sentinel 节点，由谁来做故障转移的事情呢？故障转移流程的第一步就是在 Sentinel 集群选择一个 Leader，由 Leader 完成故障转移流程。Sentinel 通过 Raft 算法，实现 Sentinel 选举。  

**问题：怎么让一个原来的 slave 节点成为主节点？**

- 选出 Sentinel Leader 之后，由 Sentinel Leader 向某个节点发送 `slaveof no one`命令，让它成为独立节点。

- 然后向其他节点发送 slaveof x.x.x.x xxxx（本机服务），让它们成为这个节点的子节点，故障转移完成。

**问题：这么多从节点，选谁成为主节点？**

关于从节点选举，一共有四个因素影响选举的结果，分别是断开连接时长、优先级排序、复制数量、进程 id。

- 如果与哨兵连接断开的比较久，超过了某个阈值，就直接失去了选举权。

- 如果拥有选举权，那就看谁的优先级高，这个在配置文件里可以设置（replica-priority 100），数值越小优先级越高。

- 如果优先级相同，就看谁从 master 中复制的数据最多（复制偏移量最大），选最多的那个

- 如果复制数量也相同，就选择进程 id 最小的那个  

#### 2 Sentinel 的功能总结  

1. 监控：Sentinel 会不断检查主服务器和从服务器是否正常运行。
2. 通知：如果某一个被监控的实例出现问题，Sentinel 可以通过 API 发出通知。
3. 自动故障转移（failover）：如果主服务器发生故障，Sentinel 可以启动故障转移过程。把某台服务器升级为主服务器，并发出通知。
4. 配置管理：客户端连接到 Sentinel，获取当前的 Redis 主服务器的地址。

#### 3 哨兵机制的不足

主从切换的过程中会丢失数据，因为只有一个 master。
只能单点写，没有解决水平扩容的问题。
如果数据量非常大，这个时候我们需要多个 master-slave 的 group，把数据分布到不同的 group 中。
问题来了，数据怎么分片？分片之后，怎么实现路由？  



## redis分布式

> Redis 3.0 版本之前，可以通过前面说所的 Redis Sentinel（哨兵）来实现高可用 ( HA )，从 3.0 版本之后，官方推出了Redis Cluster，它的主要用途是实现数据分片(Data Sharding)，同时提供了完整的 sharding、replication（复制机制仍使用原有机制，并且具备感知主备的能力）、failover 解决方案，称为 Redis Cluster，同样可以实现 HA，是官方当前推荐的方案，用来解决分布式的需求，同时也可以实现高可用。跟 Codis 不一样，它是去中心化的，客户端可以连接到任意一个可用节点。

数据分片有几个关键的问题需要解决：
1、数据怎么相对均匀地分片
2、客户端怎么访问到相应的节点和数据
3、重新分片的过程，怎么保证正常服务  

#### 1 架构

Redis Cluster 可以看成是由多个 Redis 实例组成的数据集合。客户端不需要关注数据的子集到底存储在哪个节点，只需要关注这个集合整体。以 3 主 3 从为例，节点之间两两交互，共享数据分片、节点状态等信息  

<img src="https://yliang.oss-cn-shanghai.aliyuncs.com/img/programming/image-20201216163233711.png" alt="image-20201216163233711" style="zoom: 67%;" />

#### 2 数据分布  

Redis 既没有用哈希取模，也没有用一致性哈希，而是用虚拟槽来实现的。
Redis 创建了 16384 个槽（slot），每个节点负责一定区间的 slot。比如 Node1 负责 0-5460，Node2 负责 5461-10922，Node3 负责 10923-16383。  

<img src="https://yliang.oss-cn-shanghai.aliyuncs.com/img/programming/image-20201216164437883.png" alt="image-20201216164437883" style="zoom:67%;" />

Redis 的每个 master 节点维护一个 16384 位（2048bytes=2KB）的位序列，比如：序列的第 0 位是 1，就代表第一个 slot 是它负责；序列的第 1 位是 0，代表第二个 slot不归它负责。
对象分布到 Redis 节点上时，对 key 用 CRC16 算法计算再%16384，得到一个 slot的值，数据落到负责这个 slot 的 Redis 节点上  



**问题：怎么让相关的数据落到同一个节点上？**
比如有些 multi key 操作是不能跨节点的，如果要让某些数据分布到一个节点上，例如用户 2673 的基本信息和金融信息，怎么办？

- 在 key 里面加入{hash tag}即可。Redis 在计算槽编号的时候只会获取{}之间的字符串进行槽编号计算，这样由于上面两个不同的键，{}里面的字符串是相同的，因此他们可以被计算出相同的槽。

```
user{2673}base=…
user{2673}fin=…
127.0.0.1:7293> set a{qs}a 1
OK
127.0.0.1:7293> set a{qs}b 1
OK
127.0.0.1:7293> set a{qs}c 1
OK
127.0.0.1:7293> set a{qs}d 1
OK
127.0.0.1:7293> set a{qs}e 1
OK
```

**问题：客户端连接到哪一台服务器？访问的数据不在当前节点上，怎么办？**

#### 3 客户端重定向
比如在 7291 端口的 Redis 的 redis-cli 客户端操作：
127.0.0.1:7291> set qs 1
(error) MOVED 13724 127.0.0.1:7293
服务端返回 MOVED，也就是根据 key 计算出来的 slot 不归 7191 端口管理，而是归 7293 端口管理，服务端返回 MOVED 告诉客户端去 7293 端口操作。这个时候更换端口，用 redis-cli –p 7293 操作，才会返回 OK。或者用./redis-cli -c -p port 的命令（c 代表 cluster）。这样客户端需要连接两次。Jedis 等客户端会在本地维护一份 slot——node 的映射关系，大部分时候不需要重定向，所以叫做 smart jedis（需要客户端支持）。

**问题：新增或下线了 Master 节点，数据怎么迁移（重新分配）  **

#### 3 数据迁移

因为 key 和 slot 的关系是永远不会变的，当新增了节点的时候，需要把原有的 slot分配给新的节点负责，并且把相关的数据迁移过来。
添加新节点（新增一个 7297）：
`redis-cli --cluster add-node 127.0.0.1:7291 127.0.0.1:7297`
新增的节点没有哈希槽，不能分布数据，在原来的任意一个节点上执行：
`redis-cli --cluster reshard 127.0.0.1:7291`
输入需要分配的哈希槽的数量（比如 500），和哈希槽的来源节点（可以输入 all 或者 id）。  

**问题：只有主节点可以写，一个主节点挂了，从节点怎么变成主节点？  **



#### 4 高可用和主从切换原理  

当 slave 发现自己的 master 变为 FAIL 状态时，便尝试进行 Failover，以期成为新的master。由于挂掉的master可能会有多个slave，从而存在多个slave竞争成为master节点的过程， 其过程如下：

1. slave 发现自己的 master 变为 FAIL
2. 将自己记录的集群 currentEpoch 加 1，并广播 FAILOVER_AUTH_REQUEST 信息
3. 其他节点收到该信息，只有 master 响应，判断请求者的合法性，并发送FAILOVER_AUTH_ACK，对每一个 epoch 只发送一次 ack
4. 尝试 failover 的 slave 收集 FAILOVER_AUTH_ACK
5. .超过半数后变成新 Master
6. 广播 Pong 通知其他集群节点。



> Redis Cluster 既能够实现主从的角色分配，又能够实现主从切换，相当于集成了Replication 和 Sentinal 的功能  



#### 5 总结

##### 优势

1. 无中心架构。

2. 数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。

3. 可扩展性，可线性扩展到 1000 个节点（官方推荐不超过 1000 个），节点可动态添加或删除。
   
4. 高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做 standby 数据副本，能够实现故障自动 failover，节点之间通gossip 协议交换状态信息，用投票机制完成 Slave 到 Master 的角色提升。
   
5. 降低运维成本，提高系统的扩展性和可用性。

   

##### 不足

      1. Client 实现复杂，驱动要求实现 Smart Client，缓存 slots mapping 信息并及时更新，提高了开发难度，客户端的不成熟影响业务的稳定性。
      2. 节点会因为某些原因发生阻塞（阻塞时间大于 clutser-node-timeout），被判断下线，这种 failover 是没有必要的。
   3. 数据通过异步复制，不保证数据的强一致性。
      4. 多个业务使用同一套集群时，无法根据统计区分冷热数据，资源隔离性较差，容易出现相互影响的情况  